{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- Hyperparameters ---\n",
        "D_MODEL = 64\n",
        "N_HEADS = 4\n",
        "D_FF = 128\n",
        "MAX_SEQ_LEN = 20\n",
        "N_LAYERS = 2\n",
        "\n",
        "# --- UTILITIES ---\n",
        "\n",
        "def initialize_weights(shape):\n",
        "    fan_in = shape[0]\n",
        "    limit = np.sqrt(1 / fan_in)\n",
        "    return np.random.uniform(-limit, limit, shape)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
        "\n",
        "def create_causal_mask(seq_len):\n",
        "    mask = np.triu(np.ones((seq_len, seq_len)), k=1).astype(bool)\n",
        "    return mask\n",
        "\n",
        "# --- KOMPONEN TRANSFORMER ---\n",
        "\n",
        "class TokenEmbedding:\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        self.weight = initialize_weights((vocab_size, d_model))\n",
        "    def forward(self, x):\n",
        "        return self.weight[x]\n",
        "\n",
        "def sinusoidal_positional_encoding(max_len, d_model):\n",
        "    pe = np.zeros((max_len, d_model))\n",
        "    position = np.arange(0, max_len)[:, np.newaxis]\n",
        "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
        "    pe[:, 0::2] = np.sin(position * div_term)\n",
        "    pe[:, 1::2] = np.cos(position * div_term[:d_model//2])\n",
        "    return pe\n",
        "\n",
        "class PositionalEncoding:\n",
        "    def __init__(self, max_len, d_model):\n",
        "        self.pe = sinusoidal_positional_encoding(max_len, d_model)\n",
        "    def forward(self, x):\n",
        "        seq_len = x.shape[1]\n",
        "        return x + self.pe[:seq_len, :]\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "    d_k = Q.shape[-1]\n",
        "    scores = np.matmul(Q, K.swapaxes(-2, -1)) / np.sqrt(d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "        if mask.ndim == 2:\n",
        "             mask = mask[np.newaxis, np.newaxis, :, :]\n",
        "        scores = np.where(mask, -1e9, scores)\n",
        "\n",
        "    attention_weights = softmax(scores)\n",
        "    output = np.matmul(attention_weights, V)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "class MultiHeadAttention:\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads\n",
        "        self.Wq = initialize_weights((d_model, d_model))\n",
        "        self.Wk = initialize_weights((d_model, d_model))\n",
        "        self.Wv = initialize_weights((d_model, d_model))\n",
        "        self.Wo = initialize_weights((d_model, d_model))\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        x = x.reshape(batch_size, seq_len, self.n_heads, self.d_k)\n",
        "        return x.transpose(0, 2, 1, 3).reshape(-1, seq_len, self.d_k)\n",
        "\n",
        "    def combine_heads(self, x, batch_size, seq_len):\n",
        "        x = x.reshape(batch_size, self.n_heads, seq_len, self.d_k)\n",
        "        x = x.transpose(0, 2, 1, 3)\n",
        "        return x.reshape(batch_size, seq_len, self.d_model)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        Q, K, V = [np.matmul(x, W) for W in [self.Wq, self.Wk, self.Wv]]\n",
        "\n",
        "        Q_split, K_split, V_split = [self.split_heads(v) for v in [Q, K, V]]\n",
        "\n",
        "        attn_output, attention_weights_all_heads = scaled_dot_product_attention(Q_split, K_split, V_split, mask)\n",
        "\n",
        "        attn_output_combined = self.combine_heads(attn_output, batch_size, seq_len)\n",
        "        output = np.matmul(attn_output_combined, self.Wo)\n",
        "\n",
        "        single_head_attn = np.squeeze(attention_weights_all_heads[0])\n",
        "\n",
        "        if single_head_attn.ndim > 2:\n",
        "          single_head_attn = single_head_attn[-seq_len:, -seq_len:]\n",
        "\n",
        "        return output, single_head_attn\n",
        "\n",
        "class FeedForwardNetwork:\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        self.W1 = initialize_weights((d_model, d_ff))\n",
        "        self.B1 = np.zeros((1, d_ff))\n",
        "        self.W2 = initialize_weights((d_ff, d_model))\n",
        "        self.B2 = np.zeros((1, d_model))\n",
        "\n",
        "    def forward(self, x):\n",
        "        original_shape = x.shape\n",
        "        x_flat = x.reshape(-1, original_shape[-1])\n",
        "\n",
        "        h = np.matmul(x_flat, self.W1) + self.B1\n",
        "        h = relu(h)\n",
        "\n",
        "        output_flat = np.matmul(h, self.W2) + self.B2\n",
        "        return output_flat.reshape(original_shape)\n",
        "\n",
        "class LayerNorm:\n",
        "    def __init__(self, d_model, epsilon=1e-6):\n",
        "        self.epsilon = epsilon\n",
        "        self.gamma = np.ones(d_model)\n",
        "        self.beta = np.zeros(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = np.mean(x, axis=-1, keepdims=True)\n",
        "        variance = np.var(x, axis=-1, keepdims=True)\n",
        "        x_norm = (x - mean) / np.sqrt(variance + self.epsilon)\n",
        "        return self.gamma * x_norm + self.beta\n",
        "\n",
        "class DecoderBlock:\n",
        "    def __init__(self, d_model, n_heads, d_ff):\n",
        "        self.norm1 = LayerNorm(d_model)\n",
        "        self.attn = MultiHeadAttention(d_model, n_heads)\n",
        "        self.norm2 = LayerNorm(d_model)\n",
        "        self.ffn = FeedForwardNetwork(d_model, d_ff)\n",
        "\n",
        "    def forward(self, x, causal_mask):\n",
        "        # Self-Attention (Pre-Norm + Residual)\n",
        "        x_norm1 = self.norm1.forward(x)\n",
        "        attn_output, single_head_attn = self.attn.forward(x_norm1, causal_mask)\n",
        "        x = x + attn_output\n",
        "\n",
        "        # FFN (Pre-Norm + Residual)\n",
        "        x_norm2 = self.norm2.forward(x)\n",
        "        ffn_output = self.ffn.forward(x_norm2)\n",
        "        x = x + ffn_output\n",
        "\n",
        "        return x, single_head_attn\n",
        "\n",
        "class OutputLayer:\n",
        "    def __init__(self, d_model, vocab_size, tied_weights):\n",
        "        self.W = tied_weights.T # Weight Tying\n",
        "        self.B = np.zeros((1, vocab_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        original_shape = x.shape\n",
        "        x_flat = x.reshape(-1, original_shape[-1])\n",
        "        logits_flat = np.matmul(x_flat, self.W) + self.B\n",
        "        logits = logits_flat.reshape(original_shape[0], original_shape[1], -1)\n",
        "        probabilities = softmax(logits)\n",
        "        return logits, probabilities\n",
        "\n",
        "class DecoderOnlyTransformer:\n",
        "    def __init__(self, vocab_size, d_model, n_layers, n_heads, d_ff, max_seq_len):\n",
        "        self.n_layers = n_layers\n",
        "        self.token_embed = TokenEmbedding(vocab_size, d_model)\n",
        "        self.pos_embed = PositionalEncoding(max_seq_len, d_model)\n",
        "        self.decoder_blocks = [DecoderBlock(d_model, n_heads, d_ff) for _ in range(n_layers)]\n",
        "        self.final_norm = LayerNorm(d_model)\n",
        "        self.output_layer = OutputLayer(d_model, vocab_size, self.token_embed.weight)\n",
        "\n",
        "    def forward(self, input_tokens):\n",
        "        batch_size, seq_len = input_tokens.shape\n",
        "\n",
        "        x = self.token_embed.forward(input_tokens)\n",
        "        x = self.pos_embed.forward(x)\n",
        "        causal_mask = create_causal_mask(seq_len)\n",
        "\n",
        "        attention_matrices = []\n",
        "        for block in self.decoder_blocks:\n",
        "            x, single_head_attn = block.forward(x, causal_mask)\n",
        "            attention_matrices.append(single_head_attn)\n",
        "\n",
        "        x = self.final_norm.forward(x)\n",
        "        logits, probabilities = self.output_layer.forward(x)\n",
        "\n",
        "        next_token_probs = probabilities[:, -1, :]\n",
        "\n",
        "        return logits, next_token_probs, attention_matrices\n",
        "\n"
      ],
      "metadata": {
        "id": "WB96eTaQBrCx"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text_strictly(text):\n",
        "    punc = '.,;:\"!?'\n",
        "    text = text.lower()\n",
        "    for char in punc:\n",
        "        text = text.replace(char, '')\n",
        "    return text.strip()\n",
        "\n",
        "def build_vocab_from_corpus(corpus_text):\n",
        "    tokens = [\"<PAD>\", \"<UNK>\", \"<SOS>\", \"<EOS>\"]\n",
        "\n",
        "    cleaned_corpus = clean_text_strictly(corpus_text)\n",
        "\n",
        "    # Membagi corpus menjadi kata-kata\n",
        "    corpus_tokens = cleaned_corpus.split()\n",
        "    unique_corpus_tokens = sorted(list(set(corpus_tokens)))\n",
        "\n",
        "    tokens.extend(unique_corpus_tokens)\n",
        "\n",
        "    vocab = {token: i for i, token in enumerate(tokens)}\n",
        "    id_to_token = {i: token for token, i in vocab.items()}\n",
        "\n",
        "    return vocab, id_to_token, len(tokens)\n",
        "\n",
        "def simple_tokenize(text, vocab):\n",
        "    cleaned_text = clean_text_strictly(text)\n",
        "    tokens = cleaned_text.split()\n",
        "    ids = [vocab.get(token, vocab[\"<UNK>\"]) for token in tokens]\n",
        "    return np.array([ids], dtype=np.int32)"
      ],
      "metadata": {
        "id": "2kYvNxdRF4DI"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"## SETUP TRANSFORMER INTERAKTIF (PYTHON/NUMPY ONLY) ##\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"LANGKAH 1: Masukkan Korpus untuk membangun Kosakata:\")\n",
        "corpus_input = input(\"Korpus: \\n> \")\n",
        "\n",
        "TOKEN_TO_ID, ID_TO_TOKEN, VOCAB_SIZE_TEST = build_vocab_from_corpus(corpus_input)\n",
        "\n",
        "print(f\"\\n✅ Kosakata Dibangun. Ukuran Vocab: {VOCAB_SIZE_TEST}\")\n",
        "print(f\"Token: {list(ID_TO_TOKEN.values())}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "gpt_model = DecoderOnlyTransformer(\n",
        "    vocab_size=VOCAB_SIZE_TEST,\n",
        "    d_model=D_MODEL,\n",
        "    n_layers=N_LAYERS,\n",
        "    n_heads=N_HEADS,\n",
        "    d_ff=D_FF,\n",
        "    max_seq_len=MAX_SEQ_LEN\n",
        ")\n",
        "\n",
        "print(\"\\nLANGKAH 2: Masukkan Teks Masukan untuk Prediksi Token Berikutnya:\")\n",
        "input_text = input(\"Input Teks (Contoh: Kucing suka tidur): \\n> \")\n",
        "\n",
        "input_data = simple_tokenize(input_text, TOKEN_TO_ID)\n",
        "\n",
        "if input_data.shape[1] == 0:\n",
        "    print(\"❌ ERROR: Input teks tidak mengandung token yang valid.\")\n",
        "else:\n",
        "    logits, next_token_probs, attention_matrices = gpt_model.forward(input_data)\n",
        "\n",
        "    last_token_id = input_data[0, -1]\n",
        "    last_token_str = ID_TO_TOKEN.get(last_token_id, '<UNK>')\n",
        "    INPUT_SEQ_LEN = input_data.shape[1]\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"1. LOGITS (Skor Mentah Sebelum Softmax)\")\n",
        "    print(f\"Shape: {logits.shape} (Batch=1, Seq_Len={INPUT_SEQ_LEN}, Vocab={VOCAB_SIZE_TEST})\")\n",
        "    print(f\"Logits untuk Posisi Terakhir ('{last_token_str}'):\\n{logits[0, -1, :]}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    print(\"\\n2. NEXT SOFTMAX PREDICTION (Probabilitas Token Berikutnya)\")\n",
        "    probs_vector = next_token_probs[0]\n",
        "    TOP_N = 5\n",
        "    top_indices = np.argsort(probs_vector)[::-1][:TOP_N]\n",
        "\n",
        "    print(f\"Prediksi {TOP_N} Token Teratas (setelah '{input_text}'):\")\n",
        "    print(\"-\" * 40)\n",
        "    for rank, idx in enumerate(top_indices):\n",
        "        token = ID_TO_TOKEN[idx]\n",
        "        prob = probs_vector[idx]\n",
        "        logit_value = logits[0, -1, idx]\n",
        "\n",
        "        print(f\"Rank {rank+1}: '{token}' \\t| Probabilitas: {prob*100:.3f}% \\t| Logit: {logit_value:.4f}\")\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FN4YZAZGIWc",
        "outputId": "7f0cd577-c827-4ebb-899b-610c6c8e9fd5"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "## SETUP TRANSFORMER INTERAKTIF (PYTHON/NUMPY ONLY) ##\n",
            "============================================================\n",
            "LANGKAH 1: Masukkan Korpus untuk membangun Kosakata:\n",
            "Korpus: \n",
            "> Kucing lucu suka tidur di sofa dan makan ikan\n",
            "\n",
            "✅ Kosakata Dibangun. Ukuran Vocab: 13\n",
            "Token: ['<PAD>', '<UNK>', '<SOS>', '<EOS>', 'dan', 'di', 'ikan', 'kucing', 'lucu', 'makan', 'sofa', 'suka', 'tidur']\n",
            "------------------------------------------------------------\n",
            "\n",
            "LANGKAH 2: Masukkan Teks Masukan untuk Prediksi Token Berikutnya:\n",
            "Input Teks (Contoh: Kucing suka tidur): \n",
            "> dinosaurus\n",
            "\n",
            "==================================================\n",
            "1. LOGITS (Skor Mentah Sebelum Softmax)\n",
            "Shape: (1, 1, 13) (Batch=1, Seq_Len=1, Vocab=13)\n",
            "Logits untuk Posisi Terakhir ('<UNK>'):\n",
            "[-1.11077793 -0.105291    0.90916034  0.27682617 -1.21726434 -2.02807764\n",
            "  0.10318151 -0.06852905  1.11791946  0.21859016 -0.82952143  1.30458056\n",
            "  1.30945751]\n",
            "==================================================\n",
            "\n",
            "2. NEXT SOFTMAX PREDICTION (Probabilitas Token Berikutnya)\n",
            "Prediksi 5 Token Teratas (setelah 'dinosaurus'):\n",
            "----------------------------------------\n",
            "Rank 1: 'tidur' \t| Probabilitas: 18.870% \t| Logit: 1.3095\n",
            "Rank 2: 'suka' \t| Probabilitas: 18.778% \t| Logit: 1.3046\n",
            "Rank 3: 'lucu' \t| Probabilitas: 15.581% \t| Logit: 1.1179\n",
            "Rank 4: '<SOS>' \t| Probabilitas: 12.645% \t| Logit: 0.9092\n",
            "Rank 5: '<EOS>' \t| Probabilitas: 6.719% \t| Logit: 0.2768\n",
            "----------------------------------------\n"
          ]
        }
      ]
    }
  ]
}